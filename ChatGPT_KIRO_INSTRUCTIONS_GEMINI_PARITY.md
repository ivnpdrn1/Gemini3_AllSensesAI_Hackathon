KIRO Instruction â€” Add VISION Mode (Gemini Vision) to AllSensesAI
Objective

Add Visual Context Analysis (Gemini Vision) as an automatic, safety-triggered capability to AllSensesAI.
The feature must activate only when risk is detected and must explain itself entirely through the UI, for both users in danger and jury observers.

No additional explanation should be required.
The product must communicate intent, scope, and safety on its own.

Guiding Principles (Non-Negotiable)

Vision is corroboration, not primary input

Vision is never manually activated

Vision never replaces audio or keyword triggers

Safety over spectacle

Capture minimal visual evidence

Analyze context, not identities

Explainability over raw output

No raw Gemini text dumps

Structured findings only

User = Jury

Treat both as the same first-time viewer

Everything must be obvious on screen

Where VISION Fits in the Pipeline

Keep the existing 5-step flow:

Config

Location

Voice

Analysis (Enhanced with Vision)

Alert

VISION is a sub-stage of Step 4.
Do not add a new step.

Trigger Conditions (Automatic Only)

VISION activates when ANY of the following occur:

Emergency keyword detected

Suspicious noise pattern detected (panic, scream, struggle)

Emergency state flag is set

No buttons. No toggles. No user decisions.

Step 4 â€” Threat Analysis (UI Changes Required)

Add a new panel:

ğŸ” Visual Context Analysis (Gemini Vision)

Status flow (visible):

Waiting for trigger

Capturing visual context

Analyzing environment

Analysis complete

What the system does (must be visible in text):

Captures 1â€“3 still frames

Uses front camera first, rear as fallback

Only during active emergency

No continuous recording

Gemini Vision Analysis Scope (Must Be Explicit in UI)

Show a short label such as:

â€œAnalyzing environment for safety risk indicatorsâ€

Gemini must analyze for:

Presence of other people

Signs of physical threat or coercion

Confined or isolated environments

Vehicles or enclosed spaces

Low-light or obstructed visibility

Aggressive posture or proximity

Objects that may indicate danger (non-sensational)

Prompting Rules (For KIRO Implementation)

Gemini Vision prompts must be safety-scoped.

Example instruction (conceptual, not shown to user):

â€œAnalyze this image for indicators of personal danger or distress.
Identify environmental risk factors only.
Do not identify individuals.
Respond with a structured safety assessment.â€

Visual Output (Strictly Structured)

The UI must display:

Thumbnail(s) (blurred by default)

Structured findings, e.g.:

â€œMultiple individuals detected nearbyâ€

â€œConfined indoor environmentâ€

â€œLow visibility conditionsâ€

Confidence level: Low / Medium / High

âŒ No free-form Gemini paragraphs
âŒ No emotional or speculative language

Evidence Packet Expansion (Critical)

When Vision is triggered, the emergency packet must visibly include:

ğŸ™ï¸ Trigger transcript snippet

ğŸ“ Live location (with Google Maps link)

ğŸ–¼ï¸ Visual context findings

ğŸ§  Combined risk assessment (audio + vision)

Display a label:

â€œEvidence captured to assist respondersâ€

Privacy & Trust Signals (Must Be Visible)

Without explanation text blocks, include small UI cues:

â€œImages captured only during emergencyâ€

â€œNo continuous recordingâ€

â€œSecure analysisâ€

This reassures both user and jury instantly.

Reset Behavior (Demo & Safety)

â€œReset Emergency Stateâ€ must:

Clear visual findings

Clear captured frames

Return Vision panel to idle state

Vision must not activate again until a new trigger

Acceptance Criteria (What Will Be Verified)

Say emergency keyword â†’ Step 4 auto-shows Vision panel

Visual analysis runs without user input

Findings are understandable in <5 seconds

Jury can explain what Vision does by only reading the screen

Reset clears everything cleanly

No regressions to existing workflow

Naming (Use Consistently)

Use one of the following everywhere:

Visual Context Analysis

Environmental Risk Scan

Avoid:

â€œCamera Modeâ€

â€œImage Captureâ€

â€œVision Modeâ€ (too technical)

Definition of Done

A first-time viewer can clearly understand:

why images were captured,

what was analyzed,

how it helps the person in danger,

and how it strengthens emergency response,

without anyone explaining it verbally.